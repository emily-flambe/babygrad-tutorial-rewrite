<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 6: Data Handling â€” babygrad</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <nav class="sidebar">
      <h2><a href="index.html">babygrad</a></h2>
      <ul>
        <li><a href="index.html">Preface</a></li>
        <li><a href="intro.html">1. Introduction</a></li>
        <li><a href="tensor.html">2. Tensor</a></li>
        <li><a href="autograd.html">3. Automatic Differentiation</a></li>
        <li><a href="nn.html">4. nn</a></li>
        <li><a href="optim.html">5. Optimizer</a></li>
        <li class="active"><a href="data.html">6. Data Handling</a></li>
        <li><a href="init.html">7. Initialization</a></li>
        <li><a href="saving.html">8. Model Persistence</a></li>
        <li><a href="trainer.html">9. Trainer</a></li>
        <li><a href="cnn.html">10. Convolutional NN</a></li>
        <li><a href="solutions.html">11. Solutions</a></li>
        <li><a href="examples.html">12. Examples</a></li>
        <li><a href="conclusion.html">13. Conclusion</a></li>
      </ul>
    </nav>

    <main class="content">
      <h1>Chapter 6: Data Handling</h1>

      <p>So far in our <strong>simple_mnist</strong> example, we've loaded our data and sliced it into batches manually. This is fine for a simple script, but it's not a flexible or reusable solution. What if our data is a huge collection of text files, or a massive CSV? We need a simple way to handle data.</p>

      <p>This is where the <code>Dataset</code> and <code>DataLoader</code> classes come in. They separate the problem into two parts.</p>

      <h2>6.1 Dataset</h2>

      <p>What should the <code>Dataset</code> give us?</p>
      <ul>
        <li>Length of the dataset.</li>
        <li>Retrieve data item at a given index.</li>
        <li><code>__len__</code></li>
        <li><code>__getitem__</code></li>
      </ul>

      <p><strong>FILE</strong> : <strong>babygrad/data.py</strong></p>

      <pre><code>class Dataset:
    """ Base class representing a dataset.

    This is the base class for all datasets. `__len__` method
    and the `__getitem__` method
    (which supports fetching a data sample at a given index).
    Args:
        transforms (list, optional): A list of functions that take
         a data sample and return a transformed version. Applied
        in the order they are provided. Defaults to None.
    Example:
        >>> class MyNumberDataset(Dataset):
        ...     def __init__(self, numbers):
        ...         super().__init__()
        ...         self.numbers = numbers
        ...     def __len__(self):
        ...         return len(self.numbers)
        ...     def __getitem__(self, index):
        ...         x = self.numbers[index]
                    y = x ** 2
                    return self.apply_transform(x), y
        ...
        >>> dataset = MyNumberDataset([1, 2, 3, 4])
        >>> print(f"Dataset size: {len(dataset)}")
        Dataset size: 4
        >>> print(f"Third sample: {dataset[2]}")
        Third sample: (3, 9)
    """
    def __init__(self, transforms=None):
        self.transforms = transforms
    def __getitem__(self, index):
        raise NotImplementedError
    def __len__(self):
        raise NotImplementedError
    def apply_transform(self, x):
        if not self.transforms:
            return x
        for t in self.transforms:
            x = t(x)
        return x</code></pre>

      <p><code>Dataset</code> is the Base class that we will use to create other Datasets.</p>

      <p>What are transforms here?</p>

      <p>Transforms are functions that modify data samples before they're used for training. They're applied on-the-fly when you fetch an item from the dataset.</p>

      <p>Why do we need them?</p>
      <ul>
        <li><strong>Normalization</strong> : Converting pixel values from [0, 255] to [0, 1] .</li>
        <li><strong>Data augmentation</strong> : Random crops, flips, rotations for images.</li>
        <li><strong>Type Conversion</strong> : Converting PIL images to tensors.</li>
        <li><strong>Preprocessing</strong> : Tokenizing text, applying filters, resizing images.</li>
      </ul>

      <pre><code>def normalize(x):
    """Normalize to [0, 1] range."""
    return x / 255.0

def add_noise(x):
    """Add random noise for augmentation."""
    return x + np.random.randn(*x.shape) * 0.01

# Create dataset with transforms
dataset = MyDataset(transforms=[normalize, add_noise])</code></pre>

      <h2>6.2 MNIST Dataset</h2>

      <p>Let's create our MNIST Dataset using the classes above.</p>

      <p>The <code>images</code> on a given index returns an array of size (784). We need to reshape it into (28,28,1).</p>

      <p>The index can be :</p>
      <ul>
        <li><strong>Number</strong>: Return as (28,28,1)</li>
        <li><strong>Slice</strong>: Return as (slice_length,28,28,1)</li>
      </ul>

      <p><strong>FILE</strong> : <strong>babygrad/data.py</strong></p>

      <div class="exercise">
        <h3>Exercise 6.1: Implement <code>__getitem__</code> method.</h3>

        <pre><code>class MNISTDataset(Dataset):
    def __init__(
        self,
        image_filename: str,
        label_filename: str,
        transforms: Optional[List] = None,
    ):
        self.images, self.labels = parse_mnist(image_filename=
                image_filename,label_filename=label_filename)
        self.transforms = transforms


    def __getitem__(self, index) -> object:

        #get the image and labels
        # convert to np array
        # reshape image (28,28,1) If single index else (slice_length,28,28,1)
        #apply transforms if applicable.
        # return (sample_image, sample_label)
    def __len__(self) -> int:
        return len(self.images)</code></pre>

        <div class="note">
          <p>Get the <code>parse_mnist</code> from examples.</p>
          <p>Use <code>reshape</code>.</p>
        </div>
      </div>

      <h2>6.3 Dataloader</h2>

      <p>The <code>Dataset</code> nicely gives us a single pair (x,y) of samples from the dataset when given an index.</p>

      <p>But we need samples in batches, and we also need to decide if we need them in order of their occurrence in the dataset or randomly. Random samples are important for model efficiency.</p>

      <p>So We need a <code>Dataloader</code> that will give us batches of samples from the dataset.</p>

      <p>What should <code>Dataloader</code> contain?</p>
      <ul>
        <li>Load a dataset of type <code>Dataset</code>.</li>
        <li>To shuffle or not.</li>
        <li>Batch size .</li>
      </ul>

      <pre><code>for (x,y) in dataloader(yourdataset,shuffle=True, batchsize=8):
    # do something
    #forward
    #loss
    # backward</code></pre>

      <p><strong>FILE</strong> : <strong>babygrad/data.py</strong></p>

      <ul>
        <li><code>__iter__</code>: Should initialize</li>
        <li><code>__next__</code>: Should return next sample from dataset.</li>
      </ul>

      <div class="exercise">
        <h3>Exercise 5.1</h3>

        <pre><code>class DataLoader:
    """Provides an iterator for easy batching, shuffling, and loading of
         data.
    Args:
        dataset (Dataset):
        batch_size (int, optional):
        shuffle (bool, optional):
    def __init__(self,
                 dataset: Dataset,
                batch_size: int = 1,
                shuffle: bool = True):
        self.dataset = dataset
        self.shuffle = shuffle
        self.batch_size = batch_size

    def __iter__(self):
        self.indices = np.arange(len(self.dataset))
        #   shuffle the indices if shuffle is true
        #   initialize batch idx =0
        #   get the number of batches (dataset/batch_size)
        #   return self

    def __next__(self):

        if self.batch_idx >= self.num_batches:
            raise StopIteration
        start = self.batch_idx * self.batch_size
        batch_indices = self.indices[start: start+self.batch_size]
        samples = [self.dataset[i] for i in batch_indices]

        #your solution
        # unzip the samples
        # stack them
        # wrap around Tensor and return .

        self.batch_idx+=1 </code></pre>

        <div class="note">
          <p>Use <code>np.stack</code> for stacking and convert the samples to <code>Tensor</code> before returning.</p>
        </div>
      </div>

      <div class="nav">
        <a href="optim.html">&larr; Optimizer</a>
        <a href="init.html">Next: Initialization &rarr;</a>
      </div>

      <div class="attribution">
        <p>Original: <a href="https://zekcrates.quarto.pub/deep-learning-library/data.html">zekcrates/data</a></p>
      </div>
    </main>
  </div>
  <script src="copy.js"></script>
</body>
</html>
