<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 7: Initialization - babygrad</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <nav class="sidebar">
      <h2><a href="index.html">babygrad</a></h2>
      <ul>
        <li><a href="index.html">Preface</a></li>
        <li><a href="intro.html">1. Introduction</a></li>
        <li><a href="tensor.html">2. Tensor</a></li>
        <li><a href="autograd.html">3. Automatic Differentiation</a></li>
        <li><a href="nn.html">4. nn</a></li>
        <li><a href="optim.html">5. Optimizer</a></li>
        <li><a href="data.html">6. Data Handling</a></li>
        <li class="active"><a href="init.html">7. Initialization</a></li>
        <li><a href="saving.html">8. Model Persistence</a></li>
        <li><a href="trainer.html">9. Trainer</a></li>
        <li><a href="cnn.html">10. Convolutional NN</a></li>
        <li><a href="solutions.html">11. Solutions</a></li>
        <li><a href="examples.html">12. Examples</a></li>
        <li><a href="conclusion.html">13. Conclusion</a></li>
      </ul>
    </nav>

    <main class="content">
      <h1>Chapter 7: Initialization</h1>

      <div class="note">
        <p>This is how our <strong>folder structure</strong> currently looks like. In this chapter we will work inside <strong>babygrad/init.py</strong>.</p>
        <pre><code>project/
├─ .venv/
├─ babygrad/
│   ├─ __init__.py
│   ├─ init.py
│   ├─ ops.py
│   ├─ tensor.py
│   ├─ nn.py
│   └─ optim.py
├─ examples/
│   └─ simple_mnist.py
└─ tests/</code></pre>
      </div>

      <p>Up until now, before starting training, we've been randomly initializing our weights. This seems perfectly reasonable after all, we know that by the end of the training process, gradient descent will find the optimal weights we need.</p>

      <p>So why should we even care about how we initialize the weights? Won't training just fix any poor initial choices anyway?</p>

      <p>This is a critical question. The answer is that while training finds the destination, initialization determines the starting point of the journey. A poor start can make that journey impossibly slow, unstable, or even prevent it from beginning at all.</p>

      <p>If we don't take into consideration the seriousness of good initialization, we are almost certain to face one of these three crippling problems:</p>

      <ul>
        <li><p><strong>The Symmetry Problem (Similar Weights)</strong>: If all weights start with the same value, every neuron in a layer learns the exact same feature. It's like having a team where every member is a perfect clone doing the same job completely defeating the purpose of having a team in the first place.</p></li>
        <li><p><strong>The Vanishing Gradient Problem (Smaller Weights)</strong>: When weights are consistently too small, the signal (and its gradient) shrinks as it passes through each layer. By the time it reaches the early layers, it's so faint that they receive no meaningful updates. The model simply refuses to learn anything.</p></li>
        <li><p><strong>The Exploding Gradient Problem (Bigger Weights)</strong>: The opposite occurs when weights are too large. The signal grows exponentially with each layer until it becomes massive. This results in huge, unstable updates during training that cause the loss to fly towards infinity, often seen as <strong>NaN</strong> values in your output.</p></li>
      </ul>

      <div class="hint">
        <h4>What is Uniform Distribution?</h4>
        <pre><code>A distribution where every number between a minimum value (a)
and a maximum value (b) has an equal and
constant probability of being chosen.</code></pre>
      </div>

      <div class="hint">
        <h4>What is Normal Distribution?</h4>
        <p>A distribution shaped like a "bell curve" where values are centered around an average (the mean). Values become progressively less likely the further they are from the mean. The width or "spread" of the curve is controlled by the standard deviation.</p>
      </div>

      <p>So what shall we do?</p>

      <p>We need to choose initializations that are healthy and not too big or too small.</p>

      <p>But how?</p>

      <h2>7.1 Xavier</h2>

      <h3>7.1.1 Xavier Uniform</h3>

      <p>W ~ U(-sqrt(6 / (fan_in + fan_out)), sqrt(6 / (fan_in + fan_out)))</p>

      <p>What are <code>fan_in</code> and <code>fan_out</code>?</p>

      <p>These terms refer to the number of input and output connections to a neuron in a layer. For a standard Linear (fully-connected) layer, the definitions are straightforward:</p>

      <ul>
        <li><p>fan_in: The number of input features to the layer.</p></li>
        <li><p>fan_out: The number of output features from the layer.</p></li>
      </ul>

      <p>For example, in a layer defined as Linear(in_features=784, out_features=256), the fan_in is 784 and the fan_out is 256.</p>

      <p><strong>File</strong>: <strong>babygrad/init.py</strong></p>

      <div class="exercise">
        <h4>Exercise 7.1: Implement <code>xavier_uniform</code> function.</h4>
        <pre><code>def xavier_uniform(fan_in: int, fan_out: int, gain: float = 1.0, **kwargs):
    """
    The weights are drawn from a uniform distribution U[-a, a], where
    a = gain * sqrt(6 / (fan_in + fan_out)).
    """
    # (-a,a) : find `a` . return Tensor
    #use Tensor.rand
    #your code </code></pre>
      </div>

      <h3>7.1.2 Xavier Normal</h3>

      <p>For a normal distribution, the goal is the same, but instead of defining a hard limit, we define the standard deviation (std) of the distribution</p>

      <p>W ~ N(0, 2 / (fan_in + fan_out))</p>

      <p><strong>File</strong>: <strong>babygrad/init.py</strong></p>

      <div class="exercise">
        <h4>Exercise 7.2: Implement <code>xavier_normal</code> function.</h4>
        <pre><code>def xavier_normal(fan_in: int, fan_out: int, gain: float = 1.0, **kwargs):
    """
    Xavier normal initialization.
    Calls Tensor.randn() with the correct standard deviation.
    """
    # std = the formula above
    #return Tensor </code></pre>
      </div>

      <h2>7.2 Kaiming</h2>

      <p>While Xavier initialization works well for <strong>tanh</strong> and <strong>sigmoid</strong> activations, a different approach is needed for ReLU activation function. Kaiming initialization adjusts the formulas to account for the properties of ReLU.</p>

      <h3>7.2.1 Kaiming Uniform</h3>

      <p>W ~ U(-sqrt(6 / fan_in), sqrt(6 / fan_in))</p>

      <div class="exercise">
        <h4>Exercise 7.3: Implement <code>kaiming_uniform</code> function.</h4>
        <pre><code>def kaiming_uniform(fan_in: int, fan_out: int **kwargs):
    """
    Kaiming uniform initialization.
    """
    #your code
    #Use Tensor.rand</code></pre>
      </div>

      <h3>7.2.2 Kaiming Normal</h3>

      <p>W ~ N(0, 2 / fan_in)</p>

      <div class="exercise">
        <h4>Exercise 7.4: Implement <code>kaiming_normal</code> function.</h4>
        <pre><code>def kaiming_normal(fan_in: int, fan_out: int **kwargs):
    """
    Kaiming uniform initialization.
    """
    #your code
    #use Tensor.randn</code></pre>
      </div>

      <div class="nav">
        <a href="data.html">&larr; Data Handling</a>
        <a href="saving.html">Next: Model Persistence &rarr;</a>
      </div>

      <div class="attribution">
        <p>Original: <a href="https://zekcrates.quarto.pub/deep-learning-library/initialization.html">zekcrates/initialization</a></p>
      </div>
    </main>
  </div>
  <script src="copy.js"></script>
</body>
</html>
