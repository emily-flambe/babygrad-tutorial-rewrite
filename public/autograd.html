<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 3: Automatic Differentiation — babygrad</title>
  <link rel="stylesheet" href="styles.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="container">
    <nav class="sidebar">
      <h2><a href="index.html">babygrad</a></h2>
      <ul>
        <li><a href="index.html">Preface</a></li>
        <li><a href="intro.html">1. Introduction</a></li>
        <li><a href="tensor.html">2. Tensor</a></li>
        <li class="active"><a href="autograd.html">3. Automatic Differentiation</a></li>
        <li><a href="nn.html">4. nn</a></li>
        <li><a href="optim.html">5. Optimizer</a></li>
        <li><a href="data.html">6. Data Handling</a></li>
        <li><a href="init.html">7. Initialization</a></li>
        <li><a href="saving.html">8. Model Persistence</a></li>
        <li><a href="trainer.html">9. Trainer</a></li>
        <li><a href="cnn.html">10. Convolutional NN</a></li>
        <li><a href="solutions.html">11. Solutions</a></li>
        <li><a href="examples.html">12. Examples</a></li>
        <li><a href="conclusion.html">13. Conclusion</a></li>
      </ul>
    </nav>

    <main class="content">
      <h1>Chapter 3: Automatic Differentiation</h1>

      <div class="revised">
        <strong>✓ Revised for clarity.</strong> This chapter has been rewritten with more detailed explanations.
      </div>

      <p>Yay! We will do some magic here. Automatic Differentiation (or Autograd) is the heart of every deep learning library. It allows us to calculate how much every single parameter in our model contributed to the final error, without us having to write the math by hand for every new model.</p>

      <div class="note">
        <p>The folder structure after the previous chapter looks like this:</p>
<pre><code>project/
├─ .venv/          # virtual environment folder
├─ babygrad/
│   ├─ tensor.py   #Tensor code
│
├─ examples/        # example scripts using baby
└─ tests/           # unit tests
</code></pre>
      </div>

      <h2>3.1 Operations</h2>

      <div class="hint">
        <strong>What are Operations?</strong>
        <p>Operations are <strong>mathematical functions</strong> (like addition, multiplication, or matrix multiplication) that transform one or more tensors into a new tensor.</p>
      </div>

      <p>Each <strong>Function</strong> class will have a</p>
      <ul>
        <li><strong>forward</strong> : How to calculate the output given some inputs.</li>
        <li><strong>backward</strong> : How to calculate the gradients using the Chain Rule.</li>
      </ul>

      <p>Lets define a Base Class .</p>

      <p><strong>File</strong>: <strong>babygrad/ops.py</strong></p>
<pre><code class="language-python">from .tensor import NDArray, Tensor
from typing import Tuple
import numpy as np
class Function:
    def __call__(self, *args):
        raise NotImplementedError()
    def forward(self, *args):
        """Computes the forward pass of the operation.
        Args:
            *args: One or more NumPy arrays
        """
        raise NotImplementedError()
    def backward(self, out_grad, node):
        """Calculates backward pass (gradients)
        Args:
            out_grad: upstream gradient flowing from output to input
            node: Value object holding inputs from forward pass
        """
        pass</code></pre>

      <p>This is simple Base class for All our Functions. All the <strong>Functions</strong> will be subclass of <strong>Function</strong>.</p>

      <p>What does the <strong>forward</strong> method return? <strong>Numpy Arrays</strong>! Should we wrap the output in a <code>Tensor</code>?</p>

      <p>Yes! because this <strong>Output</strong> will be used for further calculations and is a part of <strong>computation graph</strong>.</p>

      <p>So the <strong>forward</strong> method will :</p>
      <ul>
        <li>Compute new values.</li>
        <li>Wrap those values inside <code>Tensor</code>.</li>
      </ul>

      <p>Each mathematical operation (addition, multiplication, etc.) will need its own <code>forward</code> method, but the wrapping logic is the same for all of them.</p>

      <p>Since the wrapping logic is identical for every operation, we can put it in one shared method instead of duplicating it in every <code>forward()</code>. We'll use <code>__call__</code> for this.</p>

      <p><code>__call__</code> is a Python special method that makes an object callable like a function. When you write <code>Add()(a, b)</code>, Python runs <code>Add().__call__(a, b)</code>.</p>

      <p>Here's what <code>__call__</code> will do:</p>
      <ol>
        <li>Accept Tensor inputs</li>
        <li>Extract <code>.data</code> (the numpy arrays) from each input</li>
        <li>Call <code>forward()</code> with those numpy arrays</li>
        <li>Wrap the result in a new Tensor</li>
        <li>Record <code>_op</code> and <code>_inputs</code> for the computation graph</li>
        <li>Return the new Tensor</li>
      </ol>

      <p><strong>File</strong> : <strong>babygrad/ops.py</strong></p>
<pre><code class="language-python">class Function:
    def __call__(self, *args):                          # Takes inputs
        requires_grad = any(t.requires_grad for t in args)
        inputs_data = [t.data for t in args]            # Gets .data
        output_data = self.forward(*inputs_data)        # Calls forward
        # Wrap in Tensor
        output_tensor = Tensor(output_data, requires_grad=requires_grad)
        if requires_grad:
            output_tensor._op = self                    # Save operation
            output_tensor._inputs = args                # Save parents
        return output_tensor</code></pre>

      <div class="note">
        <p>Notice the <code>requires_grad</code> check: if any input requires a gradient, the output must too. This ensures gradient tracking propagates through the entire computation.</p>
      </div>

      <p><strong>Why store <code>_op</code> and <code>_inputs</code>?</strong></p>

      <p>The <code>forward</code> method computes the result, but that's only half the story. During the backward pass, we'll need to know:</p>
      <ul>
        <li>What operation created this tensor (<code>_op</code>)</li>
        <li>What tensors were its inputs (<code>_inputs</code>)</li>
      </ul>

      <p>Each tensor stores its "parents" and how it was created—like a family tree. This chain of history is what makes automatic differentiation possible: when we compute gradients, we walk backward through the graph, and each tensor knows exactly where to pass its gradients.</p>

      <p>Now let's define some concrete operations that subclass <code>Function</code>.</p>

      <h2>3.2 Forward Pass</h2>

      <p>The forward pass is straightforward: two inputs go through some operation and produce a single output value.</p>

      <p>The operation here could be:</p>
      <ul>
        <li><code>add</code></li>
        <li><code>mul</code></li>
        <li><code>sub</code></li>
        <li>or any other mathematically valid operation.</li>
      </ul>

      <p><strong>File</strong> : <strong>babygrad/ops.py</strong></p>
<pre><code class="language-python">import numpy as np
from .tensor import Tensor, NDArray
class Add(Function):
    def forward(self, a: NDArray , b:NDArray):
        return a+b
    def backward(self,out_grad , node):
        return out_grad,out_grad
def add(a, b):
    return Add()(a, b)  #`__call__`

class Mul(Op):
    def forward(self, a, b):
        return a * b
    def backward(self, out_grad, node):
        a,b = node._inputs
        return out_grad*b,out_grad*a
def mul(a, b):
    return Mul()(a, b)</code></pre>

      <p>Not too hard, right? We just completed the forward pass of <code>Add()</code>.</p>

      <div class="hint">
        <strong>Exercise 3.1</strong>
        <p>Write the <code>forward</code> method for all these classes:</p>
        <ul>
          <li>Sub</li>
          <li>Div</li>
          <li>Pow</li>
          <li>Transpose</li>
          <li>Reshape</li>
          <li>BroadcastTo</li>
          <li>Summation</li>
          <li>MatMul</li>
          <li>Negate</li>
          <li>Log</li>
          <li>Exp</li>
          <li>Relu</li>
          <li>Sigmoid</li>
          <li>Tanh</li>
          <li>Sqrt</li>
          <li>abs</li>
        </ul>
        <div class="note">
          <p>You should use numpy functions.</p>
        </div>
      </div>

      <h2>3.3 Integrating Functions inside the Tensor Class</h2>

      <p>We've written <code>forward</code> methods for a few functions, but how do we actually use them? How do we link them to the <code>Tensor</code> class?</p>

      <p>We need to <strong>method overload</strong> the functions. This means that whenever we perform operations like <code>x + y</code> between two <code>Tensor</code> objects, it will use our <code>Function</code> class (e.g., <code>Add</code>) under the hood instead of Python's default addition.</p>

      <p>The same approach applies to other operations (<code>*</code>, <code>-</code>, <code>/</code>, etc.).</p>

      <p><strong>File</strong> : <strong>babygrad/tensor.py</strong></p>
<pre><code class="language-python">class Tensor:
    def __add__(self, other):
        """Addition: a + b"""
        from .ops import Add
        if not isinstance(other, Tensor):
            other = Tensor(other)
        return Add()(self, other)

    def __radd__(self, other):
        """Right addition: 5 + tensor"""
        return self.__add__(other)
    </code></pre>

      <p>Now we can add two Tensors without any problem. <code>tensor_a + tensor_b</code> will just work.</p>

<pre><code class="language-python">from babygrad import Tensor
a = Tensor([1,2,3])
b = Tensor([2,3,4])
c = a+b

print(c.data)       # output data
>>> [3. 5. 7.]
print(c._op)        # which operation ? '+'
>>> &lt;babygrad.ops.Add object at 0x00000248BAB0A600&gt;
print(c._inputs)    #who are the parents? [a,b]
>>> [Tensor([1. 2. 3.], requires_grad=True), Tensor([2. 3. 4.],
     requires_grad=True)]</code></pre>

      <p><strong>File</strong>: <strong>babygrad/tensor.py</strong></p>

      <div class="hint">
        <strong>Exercise 3.2: Implement functions.</strong>
        <p>Write the following methods inside <code>Tensor</code> class.</p>
        <ul>
          <li><code>__mul__</code></li>
          <li><code>__pow__</code></li>
          <li><code>__sub__</code></li>
          <li><code>__truediv__</code></li>
          <li><code>__matmul__</code></li>
          <li><code>matmul</code></li>
          <li><code>sum</code></li>
          <li><code>broadcast_to</code></li>
          <li><code>reshape</code></li>
          <li><code>__neg__</code></li>
          <li><code>transpose</code></li>
        </ul>
        <p>Each operator should use the corresponding <code>Function</code> class (e.g., <code>Add</code>, <code>Mul</code>, <code>MatMul</code>).</p>
      </div>

      <h2>3.4 Backward Pass</h2>

      <p>In the Forward Pass, data flows from inputs to outputs to get a result. In the Backward Pass, we go in reverse. We start with the final error and walk backward through our family tree to figure out: "How much is each parent responsible for this error?"</p>

      <div class="hint">
        <strong>What is a gradient?</strong>
        <p>A measure of how much the final error changes when we change a specific input or weight.</p>
      </div>

      <p><strong>Local derivatives</strong> are the heart of the <strong>chain rule</strong>. The "magic" of autograd is that it breaks a massive problem into small, manageable pieces using local derivatives.</p>

      <p>Instead of trying to solve for a 100-layer neural network all at once, we solve it one operation at a time. Each <code>Function</code> class only needs to know its own local derivative. When we chain them together during the backward pass, these local pieces multiply together via the chain rule to give us the global gradient for every parameter in the system.</p>

      <p><strong>Example</strong></p>

<pre><code class="language-python">a = babygrad.Tensor([1, 2, 3], dtype="float32")
b = babygrad.Tensor([2, 3, 4], dtype="float32")

c = a + b
d = c + babygrad.Tensor([3, 5, 3], dtype="float32")</code></pre>

      <p>$$\frac{\partial d}{\partial a} = \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a}$$</p>

      <p>$$\frac{\partial d}{\partial b} = \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial b}$$</p>

      <p>As we said, gradients flow from right to left. The question is: what is the gradient of the output?</p>

      <p>$$\frac{\partial d}{\partial d} = 1$$</p>

      <p>It is simply just <strong>1</strong>. This is the <strong>upstream derivative</strong>. This upstream gradient of 1 flows backward into c. Then c applies its local derivative (\(\frac{\partial c}{\partial a} = 1\), \(\frac{\partial c}{\partial b} = 1\)) and passes gradients further to a and b.</p>

      <p>Let's look at the backward pass of <code>Add</code>:</p>

<pre><code class="language-python">class Add(Function):
    def forward(self, a: NDArray, b: NDArray):
        return a + b

    def backward(self, out_grad: Tensor, node: Tensor):
        # derivative of (a + b) wrt a is 1
        # derivative of (a + b) wrt b is 1
        # this is local derivative
        return out_grad, out_grad
class Mul(Function):
    def forward(self, a: NDArray, b: NDArray):
        return a * b

    def backward(self, out_grad: Tensor, node: Tensor):
        a, b = node._inputs
        # derivative of (a*b) wrt a is b
        # derivative of (a*b) wrt b is a
        return out_grad * b, out_grad * a
</code></pre>

      <p>The <strong>out_grad</strong> would be the <strong>upstream gradient</strong> coming from the output.</p>

      <p><strong>File</strong>: <strong>baby/ops.py</strong></p>

      <div class="hint">
        <strong>Exercise 3.3: Implement Backward Pass</strong>
        <p>Write the <code>backward</code> method for all these classes:</p>
        <ul>
          <li>Mul</li>
          <li>Sub</li>
          <li>Div</li>
          <li>Pow</li>
          <li>Negate</li>
          <li>Log</li>
          <li>Exp</li>
          <li>Relu</li>
          <li>Sigmoid</li>
          <li>Tanh</li>
          <li>Sqrt</li>
          <li>Abs</li>
        </ul>
      </div>

      <div class="hint">
        <p><strong>Guidelines:</strong></p>
        <ul>
          <li>Do <strong>not</strong> use NumPy functions directly in the backward pass.</li>
          <li>You can (and should) reuse the functions you implemented in the forward pass, e.g., <code>add()</code>, <code>mul()</code>, etc.</li>
          <li>Example:</li>
        </ul>
<pre><code class="language-python">def divide(a, b):
    return Div()(a, b)</code></pre>
        <p>You can use <strong>divide()</strong> freely in the backward pass of another operation.</p>
        <ul>
          <li>Make sure the gradients have the same shape as the corresponding input Tensor, especially for operations involving broadcasting or reshaping.</li>
        </ul>
      </div>

      <hr>

      <p>The above functions are easy to implement. Now let's tackle the more interesting ones.</p>

      <h3>3.4.1 Reshape</h3>

      <p><code>Reshape</code> is the simplest shape-changing operation. In the forward pass, we take the data and arrange it into a new shape (e.g., turning a \(1 \times 6\) vector into a \(2 \times 3\) matrix). Under the hood, it's not creating new memory—it's just changing how it views that data.</p>

      <p>$$\begin{aligned}
\text{Original matrix } A &= \begin{pmatrix}
1 & 2 & 3 & 4 \\
5 & 6 & 7 & 8 \\
9 & 10 & 11 & 12
\end{pmatrix}_{3 \times 4} \\[1em]
\text{Reshape to } B &= \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
10 & 11 & 12
\end{pmatrix}_{4 \times 3} \\[1em]
\text{or } C &= \begin{pmatrix}
1 & 2 \\
3 & 4 \\
5 & 6 \\
7 & 8 \\
9 & 10 \\
11 & 12
\end{pmatrix}_{6 \times 2}
\end{aligned}$$</p>

      <ul>
        <li>Shape of <strong>A</strong> is (3,4)</li>
        <li>In the <strong>forward pass</strong> we reshape it to (4,3).</li>
        <li>Now <code>out_grad</code> is of shape (4,3).</li>
      </ul>

      <p>We need to convert the shape of <code>out_grad</code> back to the shape of <strong>A</strong>. To pass this gradient back to the input, we must ensure the shapes match. Since reshaping doesn't change the values—only their positions—the backward pass is just a reshape in the opposite direction.</p>

      <div class="hint">
        <strong>Exercise 3.4</strong>
        <p>Let's write the <code>Reshape</code> class.</p>
<pre><code class="language-python">
class Reshape(Function):
    def __init__(self, shape):
        self.shape = shape

    def forward(self, a):
        # use np.reshape or a.reshape

    def backward(self, out_grad, node):
        a = node._inputs[0]
        #Your solution

def reshape(a, shape):
    return Reshape(shape)(a)</code></pre>
        <div class="note">
          <p>We can use <code>np.reshape</code> and also <code>a.reshape</code> as <strong>a</strong> is Numpy array. Can we use <code>reshape</code> in backward too?</p>
        </div>
      </div>

      <hr>

      <h3>3.4.2 Transpose</h3>

      <p>Transpose flips the axes of a tensor. In a 2D matrix, rows become columns and columns become rows.</p>

      <p>$$\begin{aligned}
A &= \begin{pmatrix}
a & b & c \\
d & e & f
\end{pmatrix}_{2 \times 3} \\[1em]
A^T &= \begin{pmatrix}
a & d \\
b & e \\
c & f
\end{pmatrix}_{3 \times 2}
\end{aligned}$$</p>

      <p>The input axes can:</p>
      <ul>
        <li>Have no axes specified—in that case, we transpose the last two axes.</li>
        <li>Transpose whatever axes are provided.</li>
      </ul>

      <p><strong>Backward Pass</strong></p>

      <p>What about the backward pass? For a 2D matrix, the logic is simple: \((A^T)^T = A\). To get the gradient back to the original shape, we just transpose it again.</p>

      <p>$$(A^T)^T = A$$</p>

      <p>For the backward pass, a simple transpose will work for 2D matrices. For ND matrices, we need a slightly different approach.</p>

      <p>Let's say we had a matrix:</p>
      <ul>
        <li>Original Axes order: (0, 1, 2) and Shape: (3, 2, 5)</li>
        <li>Forward Pass: Transpose(1, 2, 0) and Shape: (2, 5, 3)</li>
        <li>The Goal: Move the gradient from (2, 5, 3) back to (3, 2, 5)</li>
      </ul>

      <p>For any ND matrix that has been transposed, we just need to know where the original axes ended up.</p>

      <p>Current axis order: (1, 2, 0) with shape (2, 5, 3). Where did each original axis go?</p>
      <ul>
        <li>Where is 0 in the current order? At position 2.</li>
        <li>Where is 1 in the current order? At position 0.</li>
        <li>Where is 2 in the current order? At position 1.</li>
      </ul>

      <p>The result: (2, 0, 1). If we apply <code>transpose</code> to <code>out_grad</code> using this result, we get the original shape back.</p>

      <p>How do we compute this? <code>np.argsort</code> gives us exactly what we need. If we apply <code>np.argsort((1, 2, 0))</code>, we get <strong>(2, 0, 1)</strong>.</p>

      <div class="hint">
        <strong>Exercise 3.5</strong>
        <p>Let's write the <code>Transpose</code> class.</p>
<pre><code class="language-python">class Transpose(Function):
    def __init__(self, axes: Optional[tuple] = None):
        self.axes = axes

    def forward(self, a):
        # use np.transpose or np.swapaxes
        # Case 1: self.axes is None (swap last two axes)
        # Case 2: self.axes is not None (transpose specified axes)
    def backward(self, out_grad, node):
        # your solution
        # If axes is None just do tranpose(out_grad,axes)
        # Hint: For ND arrays, look into np.argsort to find the inverse.


def transpose(a, axes=None):
    return Transpose(axes)(a)
</code></pre>
        <div class="note">
          <p>Use <code>np.argsort</code> to find the axes order.</p>
        </div>
      </div>

      <h3>3.4.3 Summation</h3>

      <p>Sum the elements along a given axis.</p>

      <p>$$\begin{aligned}
A &= \begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{pmatrix}_{2 \times 3} \\[1em]
\text{Sum all elements: } & \sum_{i,j} A_{ij} = 21 \\[1em]
\text{Sum over rows (axis=0): } & \begin{pmatrix}
1+4 & 2+5 & 3+6
\end{pmatrix} = \begin{pmatrix}
5 & 7 & 9
\end{pmatrix} \\[1em]
\text{Sum over columns (axis=1): } & \begin{pmatrix}
1+2+3 \\
4+5+6
\end{pmatrix} = \begin{pmatrix}
6 \\
15
\end{pmatrix}
\end{aligned}$$</p>

      <p>Notice that whatever axis we sum over, that axis vanishes (or becomes 1).</p>

      <p><strong>Backward pass</strong></p>

      <p>In the backward pass, <code>out_grad</code> is smaller than the original input. To pass gradients back to the parents, we have to stretch the gradient back to the original shape.</p>

      <p>Think of it like this:</p>
      <ul>
        <li><strong>Reshape:</strong> Put "1" back into the axis that vanished so the dimensions align.</li>
        <li><strong>Multiply:</strong> Multiply the reshaped array by an array of ones with the parent's shape.</li>
      </ul>

      <p>Example with shape (3, 3):</p>
      <ul>
        <li><code>axis=None</code>: Result has shape (1,). Reshape to (1, 1), then multiply (1, 1) * (3, 3).</li>
        <li><code>axis=0</code>: Result has shape (3,). Reshape to (1, 3), then multiply (1, 3) * (3, 3).</li>
        <li><code>axis=1</code>: Result has shape (3,). Reshape to (3, 1), then multiply (3, 1) * (3, 3).</li>
      </ul>

      <div class="hint">
        <strong>Exercise 3.6</strong>
        <p>Let's write the <code>Summation</code> class.</p>
<pre><code class="language-python">class Summation(Function):
    def __init__(self, axes: Optional[tuple] = None):
        self.axes = axes
    def forward(self, a):
        # can we use np.sum?
    def backward(self, out_grad, node):
        #your solution


def summation(a, axes=None):
    return Summation(axes)(a)
</code></pre>
        <div class="note">
          <p>You can implement backward using <code>broadcast_to</code> if you have completed it or using the above simple way.</p>
        </div>
      </div>

      <h3>3.4.4 BroadcastTo</h3>

      <p>In an ideal world, all tensors would have the same shape. But in practice, we often need to broadcast a matrix of shape (3, 1) to (3, 3).</p>

      <p>$$\begin{bmatrix}
a \\
b \\
c
\end{bmatrix}
\rightarrow
\begin{bmatrix}
a & a & a \\
b & b & b \\
c & c & c
\end{bmatrix}$$</p>

      <p>We "stretch" the smaller tensor to match the larger one. During the backward pass, we do the exact opposite.</p>

      <p>If one value was used three times to produce the output, it is responsible for the error at all three of those locations. Therefore, we sum the <strong>out_grad</strong> along the axes that were stretched.</p>

      <p>When broadcasting, NumPy does two things:</p>
      <ul>
        <li><strong>Prepending:</strong> (3,) broadcast to (2, 3) adds axes to the front.</li>
        <li><strong>Stretching:</strong> If any dimension is 1, like (3, 1), it repeats to match, e.g., (3, 10).</li>
      </ul>

      <p><strong>Handling Prepending:</strong> If <code>out_grad</code> has more dimensions than the input, the forward pass added dimensions to the front. Sum <code>out_grad</code> along axis 0 until the number of dimensions matches.</p>

      <p><strong>Handling Stretching:</strong> Once dimensions match (e.g., both are 2D), you might still have a shape mismatch. If the input was (3, 1) but the gradient is (3, 10):</p>
      <ul>
        <li>If the original dimension was 1 but <code>out_grad</code>'s dimension is greater than 1, stretching happened on that axis.</li>
        <li>Sum over that axis.</li>
        <li>Be careful: summing sometimes gives (3,) instead of (3, 1). You may need to insert a 1 to restore the shape.</li>
      </ul>

      <div class="hint">
        <strong>Exercise 3.7</strong>
        <p>Let's write the <code>BroadcastTo</code> class.</p>
<pre><code class="language-python">
class BroadcastTo(Function):
    def __init__(self, shape):
        self.shape = shape

    def forward(self, a):
        # Can we use np.broadcast_to ?
    def backward(self, out_grad, node):
        # your solution

def broadcast_to(a, shape):
    return BroadcastTo(shape)(a)
</code></pre>
        <div class="note">
          <p>You can implement backward using <code>summation</code> and <code>reshape</code> methods .</p>
        </div>
      </div>

      <h3>3.4.5 Matmul</h3>

      <p>Matrix multiplication is one of the most important operations in deep learning.</p>

      <p>In the forward pass, we compute \(C = A \cdot B\). How do we distribute the gradient (\(out\_grad\)) back to \(A\) and \(B\)?</p>
      <ul>
        <li>Input \(A\) has shape \((M, N)\).</li>
        <li>Input \(B\) has shape \((N, P)\).</li>
        <li>Output \(C\) (and therefore \(out\_grad\)) has shape \((M, P)\).</li>
      </ul>

      <p><strong>Finding the Gradient for \(A\)</strong></p>

      <p>We know the result, <code>grad_a</code>, must match the shape of \(A\), which is \((M, N)\).</p>
      <ul>
        <li>We have out_grad: \((M, P)\)</li>
        <li>What can we multiply by to get \((M, N)\)?</li>
        <li>We need something with shape \((P, N)\).</li>
        <li>Looking at our inputs, \(B\) is \((N, P)\). Its transpose \(B^T\) is \((P, N)\)!</li>
      </ul>

      <p>$$\frac{\partial L}{\partial A} = out\_grad \cdot B^T$$</p>

      <p><strong>Finding the Gradient for \(B\)</strong></p>

      <p>We know grad_b must match the shape of \(B\), which is \((N, P)\).</p>
      <ul>
        <li>We have out_grad: \((M, P)\)</li>
        <li>What can we multiply by to get \((N, P)\)?</li>
        <li>We need something with shape \((N, M)\).</li>
        <li>Input \(A\) is \((M, N)\). Its transpose \(A^T\) is \((N, M)\)!</li>
      </ul>

      <p>$$\frac{\partial L}{\partial B} = A^T \cdot out\_grad$$</p>

      <div class="hint">
        <strong>Exercise 3.8</strong>
        <p>Let's write the <code>MatMul</code> class.</p>
<pre><code class="language-python">
class MatMul(Function):
    def forward(self, a, b):
        # can you use np.matul?
    def backward(self, out_grad, node):
        # your solution


def matmul(a, b):
    return MatMul()(a, b)

</code></pre>
        <div class="note">
          <p>You can implement backward using <code>summation</code> , <code>reshape</code> , <code>broadcast_to</code> and <code>matmul</code> methods .</p>
        </div>
      </div>

      <h3>3.4.6 Scalar Operations</h3>

      <p>Suppose we have</p>

<pre><code class="language-python">x = babygrad.autograd.Tensor([1,2,3])
y =x+1 </code></pre>

      <p>Here <code>x</code> is a Tensor and <code>1</code> is a scalar. We need a way to handle these mixed-type operations.</p>

      <p>Scalar operations are straightforward. The gradient only flows to the tensor input; the scalar is treated as a constant.</p>

<pre><code class="language-python">
class AddScalar(Function):
    def __init__(self, scalar):
        self.scalar = scalar

    def forward(self, a: NDArray):
        return a + self.scalar

    def backward(self, out_grad: Tensor, node: Tensor):
        return out_grad


def add_scalar(a, scalar):
    return AddScalar(scalar)(a)
</code></pre>

      <p>That's it!</p>

      <div class="hint">
        <strong>Exercise 3.9: Implement Forward and Backward Pass</strong>
        <p>Write the <code>forward</code> and <code>backward</code> method for all these classes:</p>
        <ul>
          <li>MulScalar</li>
          <li>DivScalar</li>
          <li>PowerScalar</li>
        </ul>
      </div>

      <h2>3.5 Implementing Backward Pass in the <code>Tensor</code> Class</h2>

      <p>We've built a <code>Tensor</code> class that supports forward and backward operations. We've already created the bridge between our forward operations and the <code>Tensor</code> class. Now we need to create the bridge for backward operations. Each forward function in <code>ops</code> had its own method in the <code>Tensor</code> class—do we do the same for backward?</p>

      <p>You might be tempted to write a <code>backward_add()</code> or <code>backward_mul()</code> for every operation, but that would be a maintenance nightmare.</p>

      <p>Do we really need multiple <code>backward()</code> methods? No. A single <code>backward()</code> method in the Tensor class is sufficient.</p>
      <ul>
        <li>The Tensor: Tracks the "Family Tree" (who created this tensor?).</li>
        <li>The Function: Stores the "Local Rule" (how do I derive this specific operation?).</li>
      </ul>

      <p>When you call <code>out.backward()</code>, the Tensor simply walks the graph in reverse order. At each step, it asks the parent's Function: "Here is the gradient from above; what is my share of the responsibility?"</p>

      <p>By convention, the gradient of the final output starts as 1:</p>

<pre><code class="language-python"># Start of backpropagation
all_nodes.grad = None
output.grad = 1
output.backward()</code></pre>

      <p>Let's see how to implement the <code>backward()</code> method in <code>Tensor</code>.</p>

      <p><strong>File</strong> : <strong>babygrad/tensor.py</strong></p>

<pre><code class="language-python">
class Tensor:
    def backward(self, grad=None):
        grads = {}
        if grad is None:
            #This is the upstream gradient.
            grads[id(self)] = Tensor(np.ones_like(self.data))
            # The output grad must be ones.


        topo_order = []
        visited = set()
        def build_topo(node):
            #your code

        build_topo(self)
        grads = {}
        for node in reversed(topo_order):
            #your code </code></pre>

      <p>To implement this, we need to:</p>
      <ul>
        <li>Find all nodes (Tensors) in the computation graph that contributed to the result.</li>
        <li>Order them correctly so we don't calculate a parent's gradient until we've finished calculating all of its children's gradients.</li>
      </ul>

      <p>Let's start simple. First, we'll find all the nodes in the computation graph using <strong>depth-first search</strong>.</p>

      <p>Depth-first search is an algorithm that explores a graph by going as deep as possible before backtracking.</p>

<pre><code class="language-python">def dfs(node, order, visited):
    if node in visited:
        return  # already processed

    for parent in node._inputs:
        dfs(parent, order, visited)

    visited.add(node)
    order.append(node)
</code></pre>

      <div class="note">
        <p>When backpropagating, a node in the computation graph might receive gradient contributions from multiple paths. That means a node's <code>.grad</code> might temporarily be a list of tensors—one from each path.</p>
        <p>Example:</p>
<pre><code class="language-python">a.grad = [Tensor([1, 2, 3]), Tensor([3, 4, 5]), Tensor([4, 5, 6])]</code></pre>
        <p>This is fine temporarily, but eventually we need a single tensor representing the total gradient. We need to accumulate the gradients.</p>
      </div>

      <p>You might have noticed <code>grads = {}</code> in the backward code. Why do we use a dictionary?</p>

      <p>In our backward loop, when we're at a child node, we calculate gradients for its parents. But we don't store them in <code>parent.grad</code> immediately—those parents might receive more gradients from other children. The parents will be processed later in the loop, so we need somewhere to hold their gradients until then.</p>

      <p>The <code>grads</code> dictionary is our temporary storage:</p>
      <ul>
        <li><strong>Key:</strong> The Tensor's unique ID (<code>id(node)</code>)</li>
        <li><strong>Value:</strong> The accumulated gradient so far</li>
      </ul>

      <p>The <code>grads</code> dictionary is our ledger for the entire backward pass.</p>

      <p>In the loop, when we're at a child node:</p>
      <ul>
        <li>Grab the child's gradient from the <code>grads</code> dictionary.</li>
        <li>Call <code>_op.backward(child_grad, child_node)</code> to compute the parents' gradients.</li>
        <li>Store those results back in the dictionary under each parent's <code>id()</code>.</li>
      </ul>

      <p>When the loop eventually reaches those parents, their gradients will already be waiting in the dictionary.</p>

      <hr>

      <div class="hint">
        <strong>Exercise 3.10: Implement the <code>backward</code> method</strong>
        <p>Write the <code>backward</code> method in the <code>Tensor</code> class.</p>
<pre><code class="language-python"># imports
class Tensor:
    #code
    def backward(self, grad=None):
        grads = {}
        if grad is None:
            #This is the upstream gradient.
            #The output grad must be ones.
            grads[id(self)] = Tensor(np.ones_like(self.data))


        topo_order = []
        visited = set()
        def build_topo(node):
            #your code

        build_topo(self)

        for node in reversed(topo_order):
            #get the out_grad of the node
            out_grad = grads.get(id(node))
            # parent_grads = call node._op.backward(out_grad,node)

            # for each parent inside node._inputs:
            # store grad[parent_id] = parent_grads
</code></pre>
      </div>

      <div class="nav">
        <a href="tensor.html">&larr; Tensor</a>
        <a href="nn.html">Next: Neural Network Modules &rarr;</a>
      </div>

      <div class="attribution">
        <p>Original: <a href="https://zekcrates.quarto.pub/deep-learning-library/chapter2.html">zekcrates/chapter2</a></p>
      </div>
    </main>
  </div>
  <script src="copy.js"></script>
</body>
</html>
