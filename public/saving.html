<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 8: Model Persistence — babygrad</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <nav class="sidebar">
      <h2><a href="index.html">babygrad</a></h2>
      <ul>
        <li><a href="index.html">Preface</a></li>
        <li><a href="intro.html">1. Introduction</a></li>
        <li><a href="tensor.html">2. Tensor</a></li>
        <li><a href="autograd.html">3. Automatic Differentiation</a></li>
        <li><a href="nn.html">4. nn</a></li>
        <li><a href="optim.html">5. Optimizer</a></li>
        <li><a href="data.html">6. Data Handling</a></li>
        <li><a href="init.html">7. Initialization</a></li>
        <li class="active"><a href="saving.html">8. Model Persistence</a></li>
        <li><a href="trainer.html">9. Trainer</a></li>
        <li><a href="cnn.html">10. Convolutional NN</a></li>
        <li><a href="solutions.html">11. Solutions</a></li>
        <li><a href="examples.html">12. Examples</a></li>
        <li><a href="conclusion.html">13. Conclusion</a></li>
      </ul>
    </nav>

    <main class="content">
      <h1>Chapter 8: Model Persistence</h1>

      <p>After training your model you turn off your computer and go to sleep or take a walk. When you turn your computer on again to find your model, It doesn't exist.</p>

      <p>So We need to store our model state. When storing our model should we store entire details of the model? Or the necessary ones? How can we know what is important and what is not ?</p>

      <p>When loading our model we expect to load the exact state of the model that we trained. This implies that we already have the code and architecture. So What should we save then? What is changing in a model? The <code>parameters</code> change.<br>
      So only the <code>parameters</code> must be stored.</p>

      <p>So we need a way to :</p>
      <ul>
        <li>Store the state of the model.</li>
        <li>Load the state of the model.</li>
      </ul>

      <div class="note">
        <p>This is how our <strong>folder structure</strong> currently looks like. In this chapter we will work inside <strong>babygrad/nn.py</strong>.</p>
<pre><code>project/
├─ .venv/
├─ babygrad/
│   ├─ __init__.py
│   ├─ data.py
│   ├─ init.py
│   ├─ ops.py
│   ├─ tensor.py
│   ├─ nn.py
│   └─ optim.py
├─ examples/
│   └─ simple_mnist.py
└─ tests/</code></pre>
      </div>

      <h2>8.1 Save Model</h2>

<pre><code>{
  'layer1.weight': ([[0.12, -0.53, ... ]]),
  'layer1.bias': ([0.01, 0.04, ...]),
}</code></pre>

      <p>Inside a Model We find different type of objects.</p>
      <ul>
        <li>Tensor/Parameter objects.</li>
        <li>Linear/Module objects.(Inside them we find Tensor/Parameter objects).</li>
        <li>Sequential/List/Tuple objects.</li>
      </ul>

<pre><code># Ideally, we want to save our trained model like this:
model = Sequential(Linear(10, 20), ReLU(), Linear(20, 1))
# ... train ...
model.save("my_model.pkl")</code></pre>

      <p>As mentioned above, we need to handle these 3 cases (Tensor, Module, List) and then return a dictionary object. Which consists of:</p>
      <ul>
        <li><strong>Key</strong>: A string representing the flattened path to the parameter (e.g., "<code>layer1.weight</code>" or "<code>layers.0.bias</code>"). We use dot notation to join the names as we go deeper into the recursion.</li>
        <li><strong>Value</strong>: The raw numpy array(accessed via .data). We don't want to store <code>Tensor</code> object(which has graph details) but <code>Tensor.data</code></li>
      </ul>

      <p><strong>File</strong> : <strong>babygrad/nn.py</strong></p>

      <div class="exercise">
        <h3>Exercise 8.1</h3>
        <p>Lets write the <strong>state_dict</strong> inside <strong>Module</strong> class.</p>
<pre><code>import pickle
class Module:

    def state_dict(self):
        """
        Returns a dictionary containing a whole state of the module.
        Recursively retrieves parameters from sub-modules.
        Output: dictionary{layerName: tensor.data}
        """
        #intialize a dict
        #Iterate over self.__dict__
        #If Value is  Parameter/Tensor save dict[key] = value.data
        #If Value is Module , recurse(value.state_dict()) and
                iterate over result(child_key,v) and
                store as dict[f"{key}.{child_key}] = v

        #If list/tuple, recurse if module and add key=(key.idx.child_key)
        #return dict

    def save(self,filename):
        with open(filename , 'wb') as f :
            pickle.dump(self.state_dict(), f)</code></pre>

        <div class="hint">
          <p>Don't store the <code>Tensor</code> object just store <code>tensor.data</code>.</p>
          <p>Can you use recursion here? If you find a <strong>Module</strong> , you should call <strong>state_dict()</strong> on it! .</p>
        </div>
      </div>

      <h2>8.2 Load Model</h2>

      <p>We need to load the saved model. We can't blindly copy from the dictionary of the saved model. The empty model architecture must have the same shape as the saved model architecture or else loading will not be possible.</p>

      <p>*<strong>Architecture should match</strong>.</p>

      <p>If you are loading your model state on a different architecture it won't work. Th</p>

      <ul>
        <li>For <strong>Tensor/Parameter</strong> we can directly call value.data = state_dict[key].</li>
        <li>For <strong>Module</strong> we have to understand that we <strong>saved the keys as key.childkey</strong> inside <code>state_dict</code> But the <strong><code>Module</code> doesn't care or want to know that</strong>. It only cares and has state_dict[childkey]=value.data</li>
      </ul>

      <p>So We would first like to filter our <strong><code>state_dict</code> and remove the prefix key</strong> and then call the recursion on the <strong>new_state_dict</strong>.</p>

      <p><strong>File</strong> : <strong>babygrad/nn.py</strong></p>

      <div class="exercise">
        <h3>Exercise 8.2</h3>
        <p>Lets write the <strong>load_state_dict</strong> inside <strong>Module</strong> class.</p>
<pre><code>import pickle
class Module:
    def load_state_dict(self, state_dict)-> None:
        """
        Returns a dictionary containing a whole state of the module.
        Recursively retrieves parameters from sub-modules.
        Output: Nothing
        """
        # Iterate over self.__dict__

        # Case 1: Value is Parameter/Tensor
        #   - Check if key exists in state_dict (Safety Check)
        #   - Check if shapes match
        #   - Assign value.data = state_dict[key]

        # Case 2: Value is Module
        #   - Filter state_dict for keys starting with "{key}."
        #   - Remove the prefix from the keys
        #   - Call value.load_state_dict(child_state_dict)

        # Case 3: Value is List/Tuple
        #   - Iterate index i
        #   - Filter keys starting with "{key}.{i}."
        #   - Remove prefix
        #   - Call value.load_state_dict(child_state_dict)


    def load(self, filename):
        with open(filename,'rb') as f:
            self.load_state_dict(pickle.load(f))</code></pre>

        <div class="hint">
          <p>Can you use recursion here?</p>
          <p>Just like saving, if you find a <strong>Module</strong> inside your class, you must recurse! Call <code>value.load_state_dict(child_state_dict)</code> to pass the data down</p>
          <p>Do not replace the <code>Tensor</code> object itself! Just update the numbers inside it (value.data = state_dict[key])</p>
        </div>
      </div>

      <div class="note">
        <p><strong>Warning:</strong> Our current implementation saves the model weights, which is perfect for Inference (using the model to make predictions) or Fine-Tuning (training on a new dataset).</p>
        <p>However, if you use this to pause and resume the same training run, be aware of a small catch:</p>
        <p>We are not saving the Optimizer state.</p>
        <p>Optimizers like <code>Adam</code> or <code>SGD</code> with Momentum keep track of their own internal history (running averages of gradients). When you restart the script, the Optimizer starts fresh (memory wiped). The model might take a few batches to "warm up" and stabilize again.</p>
      </div>

      <div class="nav">
        <a href="init.html">&larr; Initialization</a>
        <a href="trainer.html">Next: Trainer &rarr;</a>
      </div>

      <div class="attribution">
        <p>Original: <a href="https://zekcrates.quarto.pub/deep-learning-library/model_saving.html">zekcrates/model_saving</a></p>
      </div>
    </main>
  </div>
  <script src="copy.js"></script>
</body>
</html>
