<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 2: Tensor — babygrad</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <nav class="sidebar">
      <h2><a href="index.html">babygrad</a></h2>
      <ul>
        <li><a href="index.html">Preface</a></li>
        <li><a href="intro.html">1. Introduction</a></li>
        <li class="active"><a href="tensor.html">2. Tensor</a></li>
        <li><a href="autograd.html">3. Automatic Differentiation</a></li>
        <li><a href="nn.html">4. nn</a></li>
        <li><a href="optim.html">5. Optimizer</a></li>
        <li><a href="data.html">6. Data Handling</a></li>
        <li><a href="init.html">7. Initialization</a></li>
        <li><a href="saving.html">8. Model Persistence</a></li>
        <li><a href="trainer.html">9. Trainer</a></li>
        <li><a href="cnn.html">10. Convolutional NN</a></li>
        <li><a href="solutions.html">11. Solutions</a></li>
        <li><a href="examples.html">12. Examples</a></li>
        <li><a href="conclusion.html">13. Conclusion</a></li>
      </ul>
    </nav>

    <main class="content">
      <h1>Chapter 2: Tensor</h1>

      <div class="revised">
        <strong>✓ Revised for clarity.</strong> This chapter has been rewritten with more detailed explanations.
      </div>

      <h2>What is a Tensor, and why do we need one?</h2>

      <p>A <strong>numpy array</strong> is just data—numbers sitting in memory. It doesn't know where it came from or what created it.</p>

      <p>A <strong>Tensor</strong> is a wrapper around a numpy array that also remembers:</p>
      <ul>
        <li>Where did this data come from?</li>
        <li>What math operation created it?</li>
        <li>Should we track gradients for backpropagation?</li>
      </ul>

      <p>Think of it like a package vs. a <em>tracked</em> package. A numpy array is the item. A Tensor is the item plus its shipping history—where it's been, what happened to it along the way.</p>

      <p>This "history" is called the <strong>computation graph</strong>, and it's what makes automatic differentiation possible (which we'll cover in Chapter 3).</p>

      <h2>Tensor Attributes</h2>

      <p>Every Tensor stores these attributes:</p>

      <table>
        <tr><th>Attribute</th><th>What it is</th></tr>
        <tr><td><code>data</code></td><td>The actual numbers (always a numpy array)</td></tr>
        <tr><td><code>dtype</code></td><td>The type of the <em>elements</em> in the array (see below)</td></tr>
        <tr><td><code>grad</code></td><td>Gradients computed during backpropagation (starts as None)</td></tr>
        <tr><td><code>requires_grad</code></td><td>Should we track this tensor for gradient computation?</td></tr>
        <tr><td><code>_op</code></td><td>What operation created this tensor (None for tensors you create directly)</td></tr>
        <tr><td><code>_inputs</code></td><td>What tensors were used to create this one (empty list for tensors you create directly)</td></tr>
      </table>

      <h3>What's dtype?</h3>

      <p>A numpy array is always an <code>ndarray</code> object, but the <em>elements inside</em> can be different numeric types:</p>

      <pre><code>np.array([1, 2, 3])                  # int64 (default for integers)
np.array([1.0, 2.0, 3.0])            # float64 (default for floats)
np.array([1, 2, 3], dtype="float32") # float32 (explicit)</code></pre>

      <p>Deep learning typically uses <code>float32</code> because it's precise enough for neural network math, uses half the memory of <code>float64</code>, and GPUs are optimized for it. That's why our Tensor defaults to <code>dtype="float32"</code>.</p>

      <h3>What's _op and _inputs?</h3>

      <p>The last two (<code>_op</code> and <code>_inputs</code>) are how we build the computation graph. When you do <code>c = a + b</code>, the resulting tensor <code>c</code> will have <code>_op = "add"</code> and <code>_inputs = [a, b]</code>. That's how it "remembers" its history.</p>

      <h2>Exercise 2.1: The <code>__init__</code> method</h2>

      <h3>The Problem</h3>

      <p>Users will create Tensors in different ways:</p>

      <pre><code>Tensor([1, 2, 3])                    # from a Python list
Tensor(5)                            # from a single number
Tensor(np.array([1, 2, 3]))          # from a numpy array
Tensor(some_existing_tensor)         # from another Tensor</code></pre>

      <p>Your job: make all of these work. Internally, <code>self.data</code> should <strong>always</strong> end up as a numpy array—no matter what gets passed in.</p>

      <h3>Why does this matter?</h3>

      <p>The rest of your library assumes <code>self.data</code> is a numpy array. If it's sometimes a list, sometimes a Tensor, sometimes an array—everything breaks. So <code>__init__</code> normalizes the input into a consistent format.</p>

      <h3>Thinking through it</h3>

      <p>You need to handle three cases:</p>

      <table>
        <tr><th>What's passed in</th><th>What to do</th><th>Why</th></tr>
        <tr><td>A Tensor</td><td>Extract its <code>.data</code></td><td>A Tensor contains a numpy array inside—unwrap it</td></tr>
        <tr><td>A numpy array</td><td>Use it as-is</td><td>It's already what we want</td></tr>
        <tr><td>Anything else</td><td>Convert with <code>np.array()</code></td><td>See below</td></tr>
      </table>

      <p>The "anything else" case uses <code>np.array()</code>, which is quite flexible:</p>
      <pre><code>np.array([1, 2, 3])        # list → array([1, 2, 3])
np.array(5)                # scalar → array(5)
np.array((1, 2, 3))        # tuple → array([1, 2, 3])
np.array([[1, 2], [3, 4]]) # nested list → 2D array</code></pre>
      <p>So instead of checking for every possible type (list? tuple? int? float?), we just say "if it's not a Tensor or ndarray, hand it to numpy and let numpy figure it out." If someone passes something truly invalid (like a string), <code>np.array()</code> will either convert it to a string array or error—which is fine, because passing a string to a Tensor doesn't make sense anyway.</p>

      <h3>Starter Code</h3>

      <pre><code>import numpy as np

class Tensor:
    def __init__(self, data, *, device=None, dtype="float32", requires_grad=True):
        """
        Create a new tensor.

        Args:
            data: Array-like data (list, numpy array, or another Tensor)
            device: Device placement (currently ignored, CPU only)
            dtype: Data type for the array
            requires_grad: Whether to track gradients for this tensor
        """
        # Step 1: Normalize the input to a numpy array
        # We need to handle three cases and end up with `data` being a numpy array

        if isinstance(data, Tensor):
            # Case 1: data is a Tensor
            # We need to extract the numpy array from inside it
            # A Tensor's numpy array is stored in its .data attribute
            data = _____  # What attribute of the input Tensor contains the numpy array?

        elif isinstance(data, np.ndarray):
            # Case 2: data is already a numpy array
            # Nothing to do—it's already what we want
            data = data

        else:
            # Case 3: data is something else (list, scalar, tuple, etc.)
            # Use np.array() to convert it
            data = _____  # How do you convert something to a numpy array?

        # Step 2: Store it with the correct dtype
        # At this point, `data` is a numpy array. Convert it to the requested dtype.
        self.data = data.astype(dtype)

        # Step 3: Initialize the other attributes
        self.requires_grad = _____   # Use the requires_grad parameter passed to __init__
        self.grad = _____            # No gradient computed yet—what value represents "nothing"?
        self._device = _____         # Use the device parameter, but default to "cpu" if None
        self._op = _____             # No operation created this tensor—what value represents "nothing"?
        self._inputs = _____         # No input tensors—what's an empty collection?</code></pre>

      <div class="hint">
        <strong>Understanding <code>isinstance()</code></strong><br>
        <code>isinstance(x, SomeType)</code> returns <code>True</code> if <code>x</code> is of type <code>SomeType</code>, otherwise <code>False</code>. For example:
        <pre><code>isinstance([1, 2, 3], list)      # True
isinstance([1, 2, 3], np.ndarray) # False
isinstance(np.array([1,2,3]), np.ndarray)  # True</code></pre>
      </div>

      <h2>Exercise 2.2: Properties</h2>

      <p>These are simple one-liners. They expose information about the underlying numpy array.</p>

      <p>Implement these properties:</p>
      <ul>
        <li><code>shape</code> — return the dimensions, like <code>(3,)</code> or <code>(2, 4)</code></li>
        <li><code>ndim</code> — return the number of dimensions (1 for vector, 2 for matrix, etc.)</li>
        <li><code>size</code> — return the total number of elements</li>
        <li><code>device</code> — return the device location (stored in <code>self._device</code>)</li>
      </ul>

      <pre><code>@property
def shape(self):
    # YOUR CODE HERE

@property
def ndim(self):
    # YOUR CODE HERE

@property
def size(self):
    # YOUR CODE HERE

@property
def device(self):
    # YOUR CODE HERE</code></pre>

      <div class="hint">
        <strong>What's <code>@property</code>?</strong><br>
        It lets you access a method like an attribute—without parentheses. So instead of <code>t.shape()</code>, you can just write <code>t.shape</code>. It's syntactic sugar that makes the API cleaner.
      </div>

      <div class="hint">
        <strong>Hint:</strong> NumPy arrays already have <code>.shape</code>, <code>.ndim</code>, and <code>.size</code> attributes. Your Tensor's <code>self.data</code> is a numpy array...
      </div>

      <h2>Exercise 2.3: <code>numpy()</code> and <code>detach()</code></h2>

      <h3>numpy()</h3>

      <p>This method returns a plain numpy array, disconnected from the Tensor.</p>

      <pre><code>def numpy(self):
    """Return a copy of the data as a plain numpy array."""
    # YOUR CODE HERE</code></pre>

      <p><strong>Why return a copy?</strong> So that modifying the returned array doesn't accidentally change the Tensor's data. They stay independent.</p>

      <div class="hint">
        <strong>Hint:</strong> Use <code>.copy()</code> on a numpy array to get an independent copy.
      </div>

      <h3>detach()</h3>

      <p>This method returns a <em>new</em> Tensor with the same data, but with <code>requires_grad=False</code>.</p>

      <pre><code>def detach(self):
    """Create a new Tensor that won't track gradients."""
    # YOUR CODE HERE</code></pre>

      <p><strong>Why would you use this?</strong> Sometimes you want to use a tensor's values without including it in the computation graph. For example, when logging values during training, you don't need gradients for that.</p>

      <div class="hint">
        <strong>Hint:</strong> Create a new <code>Tensor(...)</code> with the same data but <code>requires_grad=False</code>.
      </div>

      <h2>Exercise 2.4: String Representations</h2>

      <p>Python has two ways to convert an object to a string. Implement both:</p>

      <pre><code>def __repr__(self):
    """
    Detailed view (for debugging). Shows data and requires_grad.
    Example output: Tensor([1. 2. 3.], requires_grad=True)
    """
    # YOUR CODE HERE

def __str__(self):
    """
    Simple view (for printing). Just shows the data.
    Example output: [1. 2. 3.]
    """
    # YOUR CODE HERE</code></pre>

      <p><code>__repr__</code> is what you see when you type a variable name in the REPL. <code>__str__</code> is what <code>print()</code> uses.</p>

      <div class="hint">
        <strong>Hint:</strong> Use f-strings for <code>__repr__</code>. For <code>__str__</code>, just convert <code>self.data</code> to a string.
      </div>

      <h2>What's Next?</h2>

      <p>Right now, our Tensor is just a fancy wrapper around a numpy array. The magic happens in Chapter 3, where we'll implement <code>backward()</code> and make it so that when you do math with Tensors, they automatically track their history for computing gradients.</p>

      <pre><code>def backward(self):
    # Coming in Chapter 3!
    pass</code></pre>

      <div class="nav">
        <a href="intro.html">← 1. Introduction</a>
        <a href="autograd.html">3. Automatic Differentiation →</a>
      </div>

      <div class="attribution">
        <p>Original: <a href="https://zekcrates.quarto.pub/deep-learning-library/chapter1.html">zekcrates/chapter1</a> (revised for clarity)</p>
      </div>
    </main>
  </div>
</body>
</html>
