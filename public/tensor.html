<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chapter 2: Tensor — babygrad</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <nav class="sidebar">
      <h2><a href="index.html">babygrad</a></h2>
      <ul>
        <li><a href="index.html">Preface</a></li>
        <li><a href="intro.html">1. Introduction</a></li>
        <li class="active"><a href="tensor.html">2. Tensor</a></li>
        <li><a href="autograd.html">3. Automatic Differentiation</a></li>
        <li><a href="nn.html">4. nn</a></li>
        <li><a href="optim.html">5. Optimizer</a></li>
        <li><a href="data.html">6. Data Handling</a></li>
        <li><a href="init.html">7. Initialization</a></li>
        <li><a href="saving.html">8. Model Persistence</a></li>
        <li><a href="trainer.html">9. Trainer</a></li>
        <li><a href="cnn.html">10. Convolutional NN</a></li>
        <li><a href="solutions.html">11. Solutions</a></li>
        <li><a href="examples.html">12. Examples</a></li>
        <li><a href="conclusion.html">13. Conclusion</a></li>
      </ul>
    </nav>

    <main class="content">
      <h1>Chapter 2: Tensor</h1>

      <div class="revised">
        <strong>✓ Revised for clarity.</strong> This chapter has been rewritten with more detailed explanations.
      </div>

      <h2>What is a Tensor, and why do we need one?</h2>

      <p>A <strong>numpy array</strong> is just data—numbers sitting in memory. It doesn't know where it came from or what created it.</p>

      <p>A <strong>Tensor</strong> is a wrapper around a numpy array that also remembers:</p>
      <ul>
        <li>Where did this data come from?</li>
        <li>What math operation created it?</li>
        <li>Should we track gradients for backpropagation?</li>
      </ul>

      <p>Think of it like a package vs. a <em>tracked</em> package. A numpy array is the item. A Tensor is the item plus its shipping history—where it's been, what happened to it along the way.</p>

      <p>This "history" is called the <strong>computation graph</strong>, and it's what makes automatic differentiation possible (which we'll cover in Chapter 3).</p>

      <div class="hint">
        <strong>What is a Computation Graph?</strong><br>
        A graph that shows:
        <ul>
          <li>Numbers (Tensors) as nodes</li>
          <li>Operations (add, multiply, etc.) as nodes</li>
          <li>Edges showing how data flows from inputs → operations → outputs</li>
        </ul>
        When you do <code>c = a + b</code>, the graph records: "c was created by adding a and b." Later, when computing gradients, we walk backwards through this graph.
      </div>

      <h2>Tensor Attributes</h2>

      <p>Every Tensor stores these attributes:</p>

      <table>
        <tr><th>Attribute</th><th>What it is</th></tr>
        <tr><td><code>data</code></td><td>The actual numbers (always a numpy array)</td></tr>
        <tr><td><code>dtype</code></td><td>The type of the <em>elements</em> in the array (see below)</td></tr>
        <tr><td><code>grad</code></td><td>Gradients computed during backpropagation (starts as None)</td></tr>
        <tr><td><code>requires_grad</code></td><td>Should we track this tensor for gradient computation?</td></tr>
        <tr><td><code>_op</code></td><td>What operation created this tensor (None for tensors you create directly)</td></tr>
        <tr><td><code>_inputs</code></td><td>What tensors were used to create this one (empty list for tensors you create directly)</td></tr>
      </table>

      <h3>What's dtype?</h3>

      <p>A numpy array is always an <code>ndarray</code> object, but the <em>elements inside</em> can be different numeric types:</p>

      <pre><code>np.array([1, 2, 3])                  # int64 (default for integers)
np.array([1.0, 2.0, 3.0])            # float64 (default for floats)
np.array([1, 2, 3], dtype="float32") # float32 (explicit)</code></pre>

      <p>Deep learning typically uses <code>float32</code> because it's precise enough for neural network math, uses half the memory of <code>float64</code>, and GPUs are optimized for it. That's why our Tensor defaults to <code>dtype="float32"</code>.</p>

      <h3>What's _op and _inputs?</h3>

      <p>The last two (<code>_op</code> and <code>_inputs</code>) are how we build the computation graph. When you do <code>c = a + b</code>, the resulting tensor <code>c</code> will have <code>_op = "add"</code> and <code>_inputs = [a, b]</code>. That's how it "remembers" its history.</p>

      <h2>Exercise 2.1: The <code>__init__</code> method</h2>

      <h3>The Problem</h3>

      <p>Users will create Tensors in different ways:</p>

      <pre><code>Tensor([1, 2, 3])                    # from a Python list
Tensor(5)                            # from a single number
Tensor(np.array([1, 2, 3]))          # from a numpy array
Tensor(some_existing_tensor)         # from another Tensor</code></pre>

      <p>Your job: make all of these work. Internally, <code>self.data</code> should <strong>always</strong> end up as a numpy array—no matter what gets passed in.</p>

      <h3>Why does this matter?</h3>

      <p>The rest of your library assumes <code>self.data</code> is a numpy array. If it's sometimes a list, sometimes a Tensor, sometimes an array—everything breaks. So <code>__init__</code> normalizes the input into a consistent format.</p>

      <h3>Thinking through it</h3>

      <p>You need to handle three cases:</p>

      <table>
        <tr><th>What's passed in</th><th>What to do</th><th>Why</th></tr>
        <tr><td>A Tensor</td><td>Extract its <code>.data</code></td><td>A Tensor contains a numpy array inside—unwrap it</td></tr>
        <tr><td>A numpy array</td><td>Use it as-is</td><td>It's already what we want</td></tr>
        <tr><td>Anything else</td><td>Convert with <code>np.array()</code></td><td>See below</td></tr>
      </table>

      <p>The "anything else" case uses <code>np.array()</code>, which is quite flexible:</p>
      <pre><code>np.array([1, 2, 3])        # list → array([1, 2, 3])
np.array(5)                # scalar → array(5)
np.array((1, 2, 3))        # tuple → array([1, 2, 3])
np.array([[1, 2], [3, 4]]) # nested list → 2D array</code></pre>
      <p>So instead of checking for every possible type (list? tuple? int? float?), we just say "if it's not a Tensor or ndarray, hand it to numpy and let numpy figure it out." If someone passes something truly invalid (like a string), <code>np.array()</code> will either convert it to a string array or error—which is fine, because passing a string to a Tensor doesn't make sense anyway.</p>

      <h3>Starter Code</h3>

      <pre><code>import numpy as np

class Tensor:
    def __init__(self, input, *, device=None, dtype="float32", requires_grad=True):
        """
        Create a new tensor.

        Args:
            input: Array-like input (list, numpy array, or another Tensor)
            device: Device placement (currently ignored, CPU only)
            dtype: Data type for the array
            requires_grad: Whether to track gradients for this tensor
        """
        # Step 1: Normalize the input to a numpy array

        if isinstance(input, Tensor):
            # Case 1: Unwrap the Tensor
            input = _____

        elif isinstance(input, np.ndarray):
            # Case 2: Already a numpy array
            _____

        else:
            # Case 3: Something else (list, scalar, etc.)
            input = _____

        # Store with the correct dtype
        self.data = input.astype(dtype)

        # Initialize remaining attributes
        self.requires_grad = _____
        self.grad = _____
        self._device = _____
        self._op = _____
        self._inputs = _____</code></pre>

      <div class="hint">
        <strong>Understanding <code>isinstance()</code></strong><br>
        <code>isinstance(x, SomeType)</code> returns <code>True</code> if <code>x</code> is of type <code>SomeType</code>, otherwise <code>False</code>. For example:
        <pre><code>isinstance([1, 2, 3], list)      # True
isinstance([1, 2, 3], np.ndarray) # False
isinstance(np.array([1,2,3]), np.ndarray)  # True</code></pre>
      </div>

      <h2>2.2 Data operations</h2>

      <p>Now that we have the basic Tensor class, let's extend it by adding a few <strong>simple methods</strong> that are helpful for working with tensors. For now, we'll focus on:</p>

      <ul>
        <li>Shape</li>
        <li>Dtype</li>
        <li>Device</li>
        <li>ndim</li>
        <li>Size</li>
      </ul>

      <p>These are simple properties of the Tensor class.</p>

      <pre><code>class Tensor:
    @property
    def shape(self):
        """Shape of the tensor."""
        return self.data.shape

    @property
    def dtype(self):
        """Data type of the tensor."""
        return self.data.dtype

    @property
    def ndim(self):
        """Number of dimensions."""
        return self.data.ndim

    @property
    def size(self):
        """Total number of elements."""
        return self.data.size

    @property
    def device(self):
        """Device where tensor lives."""
        return self._device</code></pre>

      <hr>

      <p>Now that we have basic properties let's focus on <strong>getting the actual data stored in a Tensor</strong>.</p>

      <p>We'll start with a simple method: <strong><code>numpy()</code></strong>.</p>

      <ul>
        <li>The purpose of <code>numpy()</code> is to <strong>extract the raw NumPy array</strong> from a Tensor.</li>
        <li>This is useful when you want to inspect, visualize, or use the data with other Python libraries without screwing anything in the graph.</li>
      </ul>

      <pre><code>class Tensor:
    # existing code...

    def numpy(self):
        """
        Return the data as a NumPy array (detached from the computation graph).
        This returns a copy, so modifying the result will not affect
        the tensor's data.

        Examples:
            >>> x = Tensor([1, 2, 3])
            >>> y = x + 1   # y is still a Tensor, part of the graph
            >>> z = x.numpy() + 1  # z is a NumPy array, not part of the graph

        Returns:
            np.ndarray: A copy of the tensor's data as a NumPy array.
        """
        return self.data.copy()</code></pre>

      <hr>

      <p>Sometimes we want a <strong>clone of a Tensor</strong> that has the same data but is <strong>not connected to the computation graph</strong>.</p>

      <p>This is useful when we want to inspect or manipulate the data without affecting the graph. For example, during training you might want to log the current loss value for monitoring, but you don't need gradients for that—you just want the number. Or you might want to "freeze" part of a model so gradients don't flow through it.</p>

      <p>The <code>detach()</code> method creates a new <strong>Tensor</strong> with the same underlying data as the original <code>Tensor</code>.</p>

      <pre><code>class Tensor:
    def detach(self):
        """
        Creates a new Tensor with same data but no gradient tracking.
        Useful when you want to use values without building
        computation graph.

        Returns:
            Tensor: New tensor with requires_grad=False

        Example:
            >>> x = Tensor([1, 2, 3], requires_grad=True)
            >>> y = x.detach()  # y doesn't track gradients
            >>> z = y * 2       # This operation won't be in graph
        """
        return Tensor(self.data, requires_grad=False)</code></pre>

      <div class="hint">
        <strong>Note:</strong> <code>detach()</code> creates a <code>Tensor</code> with <strong>requires_grad=False</strong>. That means it won't participate in the <strong>Computation graph</strong>.
      </div>

      <h2>Vocabulary</h2>

      <table>
        <tr><th>Term</th><th>What it means</th></tr>
        <tr><td><strong>Tensor</strong></td><td>A wrapper around a numpy array that also tracks its history (where it came from, what operations created it).</td></tr>
        <tr><td><strong>Gradient</strong></td><td>A number that tells you how much an output changes when you nudge an input. Used to adjust model weights during training.</td></tr>
        <tr><td><strong>Computation graph</strong> (also called "autograd graph")</td><td>The record of operations that created a tensor. Like a receipt showing the chain of math that happened.</td></tr>
        <tr><td><strong>Autograd</strong></td><td>"Automatic gradient" — the process of walking the computation graph backwards to compute gradients for you.</td></tr>
        <tr><td><strong>Forward pass</strong></td><td>Running your inputs through the model to get an output. This builds the computation graph.</td></tr>
        <tr><td><strong>Backward pass</strong></td><td>Walking the computation graph in reverse to compute gradients. This is what <code>backward()</code> does.</td></tr>
        <tr><td><strong>requires_grad</strong></td><td>A flag that says "track this tensor in the computation graph so we can compute its gradient later."</td></tr>
        <tr><td><strong>detach()</strong></td><td>Create a copy of a tensor that is NOT tracked in the computation graph.</td></tr>
      </table>

      <h2>What's Next?</h2>

      <p>Right now, our Tensor is just a fancy wrapper around a numpy array. The magic happens in Chapter 3, where we'll implement <code>backward()</code> and make it so that when you do math with Tensors, they automatically track their history for computing gradients.</p>

      <pre><code>def backward(self):
    # Coming in Chapter 3!
    pass</code></pre>

      <div class="nav">
        <a href="intro.html">← 1. Introduction</a>
        <a href="autograd.html">3. Automatic Differentiation →</a>
      </div>

      <div class="attribution">
        <p>Original: <a href="https://zekcrates.quarto.pub/deep-learning-library/chapter1.html">zekcrates/chapter1</a> (revised for clarity)</p>
      </div>
    </main>
  </div>
  <script src="copy.js"></script>
</body>
</html>
